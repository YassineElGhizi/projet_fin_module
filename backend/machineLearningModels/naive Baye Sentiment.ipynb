{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0792ea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID                                               Feed Sentiment\n",
      "0   1   اربد فيها جامعات اكثر من عمان ... وفيها قد عم...  Positive\n",
      "1   2   الحلو انكم بتحكوا على اساس انو الاردن ما فيه ...  Negative\n",
      "2   3                            كله رائع بجد ربنا يكرمك  Positive\n",
      "3   4                                 لسانك قذر يا قمامه  Negative\n",
      "4   5  ​انا داشره وغير متزوجه ولدي علاقات مشبوه واحشش...  Negative\n",
      "        ID                                               Feed Sentiment\n",
      "195    196  الدعوه مش بالكلام بس ممكن نكون دعاه باخلاقنا ا...  Positive\n",
      "1523  1524  هذه مشكله الذي يغني كثيرا في الحمام ويتصور انه...  Negative\n",
      "1754  1755            يارب يقوينا جميعا على طاعته وحسن عبادته  Positive\n",
      "1785  1786  يعني هيك برامج بتبين للناس وين صرنا كم صارت نس...  Negative\n",
      "207    208  الشعب الذي اختار نواب خرجين سجون سوابق بستاهل ...  Negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n",
    "\n",
    "data = pd.read_excel(r\"C:\\Users\\elGhizi\\Desktop\\fakeNews\\ProjetFinModule\\backend\\machineLearningModels\\AJGT.xlsx\")\n",
    "print(data.head())\n",
    "print(data.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0cdff5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID                                               Feed Sentiment\n",
      "0   1  اربد جامعات اكثر عمان وفيها عمان ونص لعيبه الم...  Positive\n",
      "1   2   الحلو انكم بتحكوا علي اساس انو الاردن فساد سرقات  Negative\n",
      "2   3                            كله راءع بجد ربنا يكرمك  Positive\n",
      "3   4                                    لسانك قذر قمامه  Negative\n",
      "4   5  ​انا داشره وغير متزوجه ولدي علاقات مشبوه واحشش...  Negative\n"
     ]
    }
   ],
   "source": [
    "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
    "\n",
    "# Arabic stop words with nltk\n",
    "stop_words = stopwords.words()\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Shadda\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    '''f\n",
    "    text is an arabic string input\n",
    "    \n",
    "    the preprocessed text is returned\n",
    "    '''\n",
    "    \n",
    "    #remove punctuations\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # remove Tashkeel\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    \n",
    "    #remove longation\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "    return text\n",
    "  \n",
    "data['Feed'] = data['Feed'].apply(preprocess)\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb267411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1052                          فاشله بيءه فاشله\n",
      "31               احيانا يكون الفشل دافع للنجاح\n",
      "110                  اقل عادي مستواك اقلب وجهك\n",
      "1216                       للمفاعل داخل الاردن\n",
      "241                    الله اكبر احلاكم اشجعكم\n",
      "                         ...                  \n",
      "1718    غبي كوادر الاخوان المسلمين بقعه هالارض\n",
      "1170                                  كلب واطي\n",
      "1778                             يعطيك العافيه\n",
      "1174          كلمه حب عجبتني جيت اقءلها جرحتني\n",
      "1736     يارب اعيني علي دكرك وشكرك وحسن عبادتك\n",
      "Name: Feed, Length: 360, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into target and feature\n",
    "feature = data.Feed\n",
    "target = data.Sentiment\n",
    "# splitting into train and tests\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(feature, target, test_size =.2, random_state=100)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c39c4a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Accuracy score is 0.84\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.76      0.82       176\n",
      "    Positive       0.80      0.91      0.85       184\n",
      "\n",
      "    accuracy                           0.84       360\n",
      "   macro avg       0.85      0.84      0.84       360\n",
      "weighted avg       0.85      0.84      0.84       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(),\n",
    "                    MultinomialNB())\n",
    "pipe.fit(X_train,Y_train)\n",
    "prediction = pipe.predict(X_test)\n",
    "print(type(X_test))\n",
    "print(f\"Accuracy score is {accuracy_score(Y_test, prediction):.2f}\")\n",
    "print(classification_report(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95771033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfvect = TfidfVectorizer(stop_words='arabic',max_df=0.7)\n",
    "tfvect = TfidfVectorizer(max_df=0.8)\n",
    "def fake_news_det(news):\n",
    "    input_data = [news]\n",
    "    vectorized_input_data = pd.Series(input_data)\n",
    "    print(type(vectorized_input_data))\n",
    "    prediction = pipe.predict(vectorized_input_data)\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c3a0f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "['Positive']\n"
     ]
    }
   ],
   "source": [
    "fake_news_det('يعطيك العافيه')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9472e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
